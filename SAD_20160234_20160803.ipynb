{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAD_20160234_20160803.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-VlgoT-337uB",
        "KWPKSTBMj1ex",
        "txmYHcLOjpyh",
        "ADh0AzTFurzo",
        "_k9vueDq4wi0",
        "CZbb6gqN61IH",
        "zTWdo_IYiZqs",
        "2_QS6Yvw44yn",
        "U8B9blYBVhxZ",
        "8D9xHzHgzMwo",
        "IHfLuc6GyV--",
        "WdWy76j61qB_",
        "wZE_l-2hXATt",
        "ktTbZiJ6cTJo",
        "RQVr_yRFWvNU",
        "GNGS-w9LHaUc",
        "qR22BFotfgUT",
        "5ER-bollI6Ux",
        "_eVzzejK0dlM",
        "h3MqiQOMr8IB",
        "xdlMX6KhsD7l",
        "YekbtQmQvqQE",
        "gTOY4ysV7BA8",
        "x4O3c7ND7Pc2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duarbert/SAD_DuarteSantos/blob/master/SAD_20160234_20160803.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-VlgoT-337uB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sistemas de Apoio à Decisão ( Run All Working )"
      ]
    },
    {
      "metadata": {
        "id": "KWPKSTBMj1ex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### TPC 5 - PARTE 1 DO TRABALHO FINAL (15 dias)\n",
        "\n",
        "---------------\n",
        "\n",
        "\n",
        "#### Com base no seu dataset (Determinado para a sua resolução do Trabalho final)\n",
        "\n",
        "1. Catalogar o dataset quanto a\n",
        "    1. Dimensionalidade\n",
        "    - Esparsidade\n",
        "    - Resolução\n",
        "    - Tamanho\n",
        "\n",
        "\n",
        "2. Catalogar todos as features (Máx 50), quanto a:\n",
        "    1. valores de valores médios, variâncias,histogramas (crie um módulo que faça isto por si);\n",
        "    - Tipo (contínuo, discreto ou binário)   \n",
        "    - Analise NaN's (ou Nulls) e Outliers\n",
        "    - Analise os resultados\n",
        "    - Quais não fazem sentido em termos do problema que pretende resolver;\n",
        "    - Que outras features poderia utilizar para enriquecer a informação do seu problema;\n",
        "    - Perceba se deve ou não fazer encoding ou one hot encoding para cada feature discreta;\n",
        "    - Faça uma matriz de correlação entre as features usando ```dataframe.corr()```;\n",
        "\n",
        "\n",
        "\n",
        "3. Analise em termos estatísticos a Label/target que tem para já como problema.\n",
        "\n",
        "---\n",
        "\n",
        "No final, faça um pequeno texto relatando o dataset que está a utilzar, recorrendo à informação que obteve nas perguntas anteriores;"
      ]
    },
    {
      "metadata": {
        "id": "txmYHcLOjpyh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####TPC 6 - PARTE 2 DO TRABALHO FINAL (Último trabalho)\n",
        "\n",
        "---------------\n",
        "\n",
        "#### Com base no seu dataset (Determinado para a sua resolução do Trabalho final)\n",
        "\n",
        "4. Aplicar 2 dos algoritmos dados nas aulas ao vosso dataset, para classificação/regressão/clustering, fazendo:\n",
        "    1. Fazendo um split training set/test set;\n",
        "    - Traçando a curva de aprendizagem do método tendo em conta uma modificação sucessiva do tamanho dos dados de treino;\n",
        "    - Comparando os resultados dos algoritmos e mostrando o que melhor funciona;\n",
        "\n",
        "***Caso Seja um Regressor***\n",
        "- Apresentar RMSE dos dois métodos e comparar resultados;\n",
        "- Rever feautures usadas e procurar melhorar o resultado obtido, testando mais/outras features;\n",
        "    \n",
        "***Caso Seja um Classificador***\n",
        "\n",
        "- Escrever a matriz de confusao resultante da aplicação iterativa do método;\n",
        "- Medir a sua Precisão e Recall;\n",
        "- Definir um contexto de uso razoável do algoritmo e perceber se nessa situação é mais relevante a precisão ou o recall;\n",
        "- De acordo com a métrica encontrada, rever feautures usadas e procurar melhorar o resultado obtido, testando mais/outras features;"
      ]
    },
    {
      "metadata": {
        "id": "ADh0AzTFurzo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Definição\n",
        "\n",
        "Este dataset tem como objectivo classificar uma dada instância de um empregado de uma empresa como potêncial candidato a sair da empresa por variadíssimas razões.\n",
        "\n",
        "*Attrition, in Human Resource terminology, refers to the phenomenon of the employees leaving the company. Attrition in a company is usually measured with a metric called attrition rate, which simply measures the no of employees moving out of the company (voluntary resigning or laid off by the company). Attrition Rate is also referred as churn rate or turnover.*\n",
        "\n",
        "*High attrition is a cause of concern for a company as it presents a cost to the company. The company loses on the amount it spent to recruit and select these employees and to train them for their respective jobs. The company may also have to spend additional money to fill the vacancies left open by these employees. Hence it becomes critical for a company to keep a tab on the attrition rate which down-sizes the employee base.*\n",
        "\n",
        "*Attrition rate gives an idea as to how many employees are leaving the company at any given time period. It is an important factor as companies have to prepare to start recruiting for the positions which are critical and cannot be left vacant.*"
      ]
    },
    {
      "metadata": {
        "id": "_k9vueDq4wi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Dependências#"
      ]
    },
    {
      "metadata": {
        "id": "rAsMaxZU5YRd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Montar a Google Drive em /content/gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KA0L7shwW4W",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Importar Módulos\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pandas.api.types import is_string_dtype\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZbb6gqN61IH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Carregar os Dados#"
      ]
    },
    {
      "metadata": {
        "id": "6D3KEKMbwZj6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Carregar o dataset e definir o índice.\n",
        "#@markdown Dataframes Gerados:\n",
        "#@markdown data_orig = Dados originais carregados do CSV.\n",
        "#@markdown data = Dados para serem mastigados.\n",
        "\n",
        "# Índice definido em EmployeeNumber\n",
        "data = pd.DataFrame(pd.read_csv('/content/gdrive/My Drive/SAD/IBMHR.csv', index_col = 'EmployeeNumber'))\n",
        "print('Criado dataset data para continuar a ser otimizado.')\n",
        "\n",
        "data_orig = data.copy()\n",
        "print('\\nCriado dataframe data_orig com os dados originais do csv.')\n",
        "data.set_axis = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "7glCwoChl4WK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title (Opcional) - O target, Attrition, não está balanceado. Vamos proceder ao undersampling da classe Attrition\n",
        "\n",
        "# Class count\n",
        "# value_counts() = Return a Series containing counts of unique values.\n",
        "#count_class_0, count_class_1 = data.Attrition.value_counts()\n",
        "\n",
        "# Dividir por classe\n",
        "#df_class_0 = data[data['Attrition'] == 'No']\n",
        "#df_class_1 = data[data['Attrition'] == 'Yes']\n",
        "\n",
        "# Undersampling\n",
        "# sample() = Return a random sample of items from an axis of object.\n",
        "#df_class_0_under = df_class_0.sample(count_class_1)\n",
        "#data = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
        "\n",
        "#print('Random under-sampling:')\n",
        "#print(df_class_0_under.Attrition.value_counts())\n",
        "\n",
        "#data.Attrition.value_counts().plot(kind='bar', title='Count (Attrition)');\n",
        "\n",
        "#print('\\n Target balanceado.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTWdo_IYiZqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Informação do Dataset - Raw"
      ]
    },
    {
      "metadata": {
        "id": "uq_AXHYoiqFr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Listar informação head do dataset \n",
        "print('!!! A feature EmployeeNumber está a ser considerada como índice, pois não há nem faz sentido haver duplicados nesta feature !!!\\n')\n",
        "display(data.head())\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "data.columns = data.columns.str.strip()\n",
        "print('\\n')\n",
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_QS6Yvw44yn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1.(está em 2.2) Catalogar o dataset quanto a#\n",
        "1. Dimensionalidade\n",
        "- Esparsidade\n",
        "- Resolução\n",
        "- Tamanho"
      ]
    },
    {
      "metadata": {
        "id": "U8B9blYBVhxZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.1 Catalogar todos as features (Máx 50), quanto a valores de:#\n",
        "1. valores médios\n",
        "- variâncias\n",
        "- histogramas (crie um módulo que faça isto por si);"
      ]
    },
    {
      "metadata": {
        "id": "bDg97M3kV-PW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Valores Estatísticos\n",
        "# Easy mode:\n",
        "# display(data.describe())\n",
        "\n",
        "\n",
        "# Like a pro\n",
        "values = [data.mean(), data.var(), data.std()]\n",
        "\n",
        "result = pd.concat(values,axis=1, join_axes=[data.mean().index])\n",
        "\n",
        "result.columns = [\"Média\",\"Variância\",\"Desvio Padrão\"]\n",
        "\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_k2b_2Eho4Nv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Histogramas - Gerados for função conforme requisito\n",
        "#Função para geração dos histogramas.\n",
        "# Usando a keyword 'All', corre todas as features\n",
        "\n",
        "def plotHist(datasetVar, columnVar):\n",
        "    if columnVar == 'All':\n",
        "      for i in datasetVar.columns:\n",
        "        if is_numeric_dtype(datasetVar[i]):\n",
        "          display('You are about to see data from feature ' + i)\n",
        "          datasetVar.hist(column = i, figsize=[12,4])\n",
        "          plt.show()\n",
        "          df3=data[i].groupby([data['Attrition']])\n",
        "          plt.ylabel('Number of Workers')\n",
        "          df3.plot(kind='hist', legend = True, title = i, alpha=0.5, figsize=[12,4])\n",
        "          plt.show()\n",
        "    else:\n",
        "        if is_numeric_dtype(datasetVar[columnVar]):\n",
        "          display('You are about to see data from feature ' + columnVar)\n",
        "          datasetVar.hist(column = columnVar, figsize=[12,4])\n",
        "          plt.show()\n",
        "          df3=data[columnVar].groupby([data['Attrition']])\n",
        "          plt.ylabel('Number of Workers')\n",
        "          df3.plot(kind='hist', legend = True, title = columnVar, alpha=0.5, figsize=[12,4])\n",
        "          plt.show()\n",
        "\n",
        "plotHist(data,'All') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8D9xHzHgzMwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.2 Tipo (contínuo, discreto ou binário)"
      ]
    },
    {
      "metadata": {
        "id": "e-PICjC5zWmL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Dimensionalidade, Esparsidade, Resolução, Tamanho. Tipo(contínuo, discreto ou binário)\n",
        "\n",
        "print('Em termos de dimensionalidade, o dataset tem ' + str(len(data.columns)) + ' features / colunas.')\n",
        "print('\\n')\n",
        "\n",
        "# As features do tipo objecto estão excluídas\n",
        "for i in data.select_dtypes(exclude='object'):\n",
        "    print ('\\nA esparsidade de valores da feature',i, 'é de:\\n', data[i].max()-data[i].min())\n",
        "\n",
        "print('\\nO tamanho(instâcias, features) é de: ' + str(data.shape))\n",
        "print('\\n')\n",
        "\n",
        "print('Resolução do datset na seguinte tabela:\\n')\n",
        "dataRes = pd.DataFrame(pd.read_csv('/content/gdrive/My Drive/SAD/IBMHR_Features - Catalog.csv'))\n",
        "display(dataRes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IHfLuc6GyV--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.7 Pré-processamento dos Dados\n",
        "- Remover feature irrelevantes\n",
        "- Binarizar de forma básica os valores de Sim e Não\n",
        "- 2.7  Onde Hot Encoding em features"
      ]
    },
    {
      "metadata": {
        "id": "7DOD8VO9_pRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "XL0IlRgvBLjw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Binarizar de forma básica, transformando valores de Sim e Não por valores inteiros 1 e 0\n",
        "\n",
        "mapping = {'Yes':1, 'No':0, 'Female':1, 'Male':0}\n",
        "data = data.replace(mapping)\n",
        "\n",
        "print('Yes, e Mulher = 1\\nNo, e Male = 0')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qztG3hZYMT3v",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title  One Hot Encoding com prefixo, pois iria originar features duplicadas\n",
        "\n",
        "aaa = pd.get_dummies(data.BusinessTravel, prefix = 'BusinessTravel')\n",
        "bbb = pd.get_dummies(data.Department, prefix = 'Department')\n",
        "ccc = pd.get_dummies(data.JobRole, prefix = 'JobRole')\n",
        "ddd = pd.get_dummies(data.MaritalStatus, prefix = 'MaritalStatus')\n",
        "eee = pd.get_dummies(data.EducationField, prefix = 'EducationField')\n",
        "\n",
        "values = [data, aaa, bbb, ccc, ddd, eee]\n",
        "\n",
        "data = pd.concat(values,axis=1, join_axes=[data.index])\n",
        "\n",
        "# Apagar colunas que foram one hotted\n",
        "\n",
        "data = data.drop(['BusinessTravel', 'Department', 'JobRole', 'MaritalStatus', 'EducationField'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PfwKLN0TQiSr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Resumo do resultado da binarização básica e one-hot-encoding\n",
        "data_pp = data\n",
        "data_pp.info()\n",
        "data_pp.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdWy76j61qB_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.5 Remover Features Irrelevantes"
      ]
    },
    {
      "metadata": {
        "id": "PqndxSkh-s2S",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Remover features irrelevantes\n",
        "\n",
        "print('As features StandardHours, Over18 e EmployeeCount são consideradas irrelevantes.')\n",
        "print('\\nA feature StandardHours tem sempre o mesmo valor de 8')\n",
        "print('\\nA feature Over18 tem sempre o mesmo valor de True')\n",
        "print('\\nA feature EmployeeCount tem sempre o mesmo valor de 1')\n",
        "\n",
        "data_pp = data_pp.drop(['StandardHours', 'Over18', 'EmployeeCount'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZE_l-2hXATt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#2.3 NaNs e Outliers##\n"
      ]
    },
    {
      "metadata": {
        "id": "czC4eWShXkUW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Valore Nulos\n",
        "display('Existem ' + str(data_pp.isnull().sum().sum()) + ' valores nulos no dataset')\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfkbXuxp3B7a",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Remoção de Outliers com 1º e 3º quantis de 1%\n",
        "Q1 = data_pp.quantile(0.01)\n",
        "Q3 = data_pp.quantile(0.99)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('A dispersão estatística dos dados tendo em conta quantis min e max de 1% \\n\\n', IQR)\n",
        "\n",
        "\n",
        "data_pp_out = data_pp[~((data_pp < (Q1 - 1.5 * IQR)) |(data_pp > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "print('\\nOutliers removidos e novo dataset criado com o nome data_pp_out')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktTbZiJ6cTJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#O código abaixo define subsets de dados:\n",
        "* Considerados sociais e independentes à IBM = data_social\n",
        "* Relativos a salários = data_salary\n",
        "* Hisórico na IBM = data_work_hist\n",
        "* Features gerais relacionadas com a função = data_work\n",
        "* Features departamentais = data_work_department\n",
        "* Features de função a desempenhar = data_work_role\n",
        "* Features de viagens em trabalho = data_work_travel\n",
        "* Features relativas a educação = data_educ\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YR8VJ4nFdEzd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Definir Subsets\n",
        "\n",
        "data_social = data_pp_out[['Age','DistanceFromHome','Gender','MaritalStatus_Divorced',\n",
        "                           'MaritalStatus_Married','MaritalStatus_Single',\n",
        "                           'RelationshipSatisfaction', 'WorkLifeBalance',\n",
        "                           'NumCompaniesWorked', 'TotalWorkingYears']].copy()\n",
        "data_social.info()\n",
        "print('\\n\\n')\n",
        "\n",
        "data_educ = data_pp_out[['Education','EducationField_Human Resources', 'EducationField_Life Sciences',\n",
        "                         'EducationField_Marketing','EducationField_Medical','EducationField_Other',\n",
        "                         'EducationField_Technical Degree']].copy()\n",
        "data_educ.info()\n",
        "print('\\n\\n')\n",
        "\n",
        "data_salary = data_pp_out[['MonthlyIncome', 'DailyRate','HourlyRate','MonthlyRate','PercentSalaryHike']].copy()\n",
        "data_salary.info()\n",
        "\n",
        "data_work = data_pp_out[['EnvironmentSatisfaction','JobInvolvement', 'JobLevel',\n",
        "                         'JobSatisfaction','PerformanceRating','StockOptionLevel',\n",
        "                         'OverTime']].copy()\n",
        "data_work.info()\n",
        "print('\\n\\n')\n",
        "\n",
        "data_work_role = data_pp_out[['JobRole_Healthcare Representative','JobRole_Human Resources',\n",
        "                              'JobRole_Laboratory Technician','JobRole_Manager',\n",
        "                              'JobRole_Manufacturing Director','JobRole_Research Director',\n",
        "                              'JobRole_Research Scientist','JobRole_Sales Executive',\n",
        "                              'JobRole_Sales Representative']].copy()\n",
        "data_work_role.info()\n",
        "print('\\n\\n')\n",
        "\n",
        "data_work_travel = data_pp_out[['BusinessTravel_Non-Travel', 'BusinessTravel_Travel_Frequently',\n",
        "                         'BusinessTravel_Travel_Rarely']].copy()\n",
        "data_work_travel.info()\n",
        "print('\\n\\n')\n",
        "\n",
        "data_work_hist = data_pp_out[['TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole',\n",
        "                              'YearsSinceLastPromotion','YearsWithCurrManager']].copy()\n",
        "data_work_hist.info()\n",
        "print('\\n\\n')\n",
        "\n",
        "data_work_department = data_pp_out[['Department_Human Resources', 'Department_Research & Development',\n",
        "                                    'Department_Sales']].copy()\n",
        "\n",
        "subsets= {'data_social':data_social, 'data_social':data_salary,\n",
        "          'data_work':data_work, 'data_work_role':data_work_role,\n",
        "          'data_work_hist':data_work_hist, \n",
        "          'data_work_department':data_work_department,\n",
        "          'data_work_travel':data_work_travel,\n",
        "          'data_educ': data_educ}\n",
        "\n",
        "\n",
        "data_work_department.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQVr_yRFWvNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.5 À partida os dados atuais fazem sentido para o problema##\n",
        "\n",
        "Depois de remover as features não desejadas por terem sempre o mesmo valor, não se dascarta à partida nenhuma feature tendo em conta o tipo de classificação e target.\n",
        "Qualquer feature relacionada com o indíviduo poderá ser relevante."
      ]
    },
    {
      "metadata": {
        "id": "GNGS-w9LHaUc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.6 As seguintes features poderiam enriquecer:\n",
        "- Número de dias com falta por doença\n",
        "- Número de dias com falta não justificada\n",
        "- Avaliações de desempenho anuais"
      ]
    },
    {
      "metadata": {
        "id": "qR22BFotfgUT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.8 Matriz de Correlação"
      ]
    },
    {
      "metadata": {
        "id": "tn2LQLlIfUAO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "corr = data_pp_out.corr()\n",
        "corr.style.background_gradient()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ER-bollI6Ux",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3 Analise em termos estatísticos a Label/target que tem para já como problema"
      ]
    },
    {
      "metadata": {
        "id": "vUHXgsiiIvbZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Copiar a coluna de attrition para uma DF temporária\n",
        "\n",
        "target_stat = data_pp_out.filter(['Attrition'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzTaT04AI5hZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Tendo em conta que é uma classificação binária, a análise irá se basear na soma e rácio de cada valor.\n",
        "\n",
        "target_count = data.Attrition.value_counts()\n",
        "print('False:', target_count[0])\n",
        "print('True:', target_count[1])\n",
        "print('Proporção:', round(target_count[0] / target_count[1], 2), ': 1')\n",
        "plt.ylabel('Nº de Instancias')\n",
        "plt.xlabel('Soma de 0:False e True:1')\n",
        "\n",
        "target_count.plot(kind='bar', title='Count (target)');\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsYRFKUYWBNF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Esparsidade das features\n",
        "for i in data_pp_out.select_dtypes(include='int64'):\n",
        "    print ('O esparsidade de valores da feature ',i, 'é de', data_pp_out[i].max()-data_pp_out[i].min())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_eVzzejK0dlM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4. Aplicação de Algoritmos"
      ]
    },
    {
      "metadata": {
        "id": "3ijyM7Ji6N96",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 1.1 Definir o SET, o Target e escolher as features\n",
        "\n",
        "df_al = data_pp_out.copy()\n",
        "y = df_al.pop('Attrition')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWTWMZHDn8e7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Opções Subset = \n",
        "* data_social; - Feature sociais\n",
        "* data_educ; - Features de educação\n",
        "* data_work;  - Features gerais do trabalho\n",
        "* data_salary;  - Features salariais\n",
        "* data_work_hist; - Features de histórico interno\n",
        "* data_work_role; - Features da função na IBM\n",
        "* data_work_department - Features do departamento\n",
        "* df_al - Todas as features"
      ]
    },
    {
      "metadata": {
        "id": "4NglD96zfrjH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Opções de Subset = data_social; data_work; data_salary; data_work_hist ou data_pp_out(todas as features)\n",
        "#go_set = data_work_department.join(data_work_role.join(data_work))\n",
        "#go_set = data_social\n",
        "go_set = data_work\n",
        "#go set = data_work.join(work_salary)\n",
        "\n",
        "features = list(go_set.columns[0:49])\n",
        "X = go_set[features]\n",
        "go_set.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3UrkxrKRhFu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 4.1 Split do dataset, train e test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(go_set, y, test_size = 0.2, random_state = 5)\n",
        "print('Features de treino(instâncias, features) ', X_train.shape)\n",
        "print('Features de teste(instâncias, features) ', X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Vyye29GbINx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Função da Matriz Confusão\n",
        "\n",
        "def get_confusion_matrix(y_true, y_pred):\n",
        "    n_classes = len(np.unique(y_true))\n",
        "    conf = np.zeros((n_classes, n_classes))\n",
        "    for actual, pred in zip(y_true, y_pred):\n",
        "        conf[int(actual)][int(pred)] += 1\n",
        "    return conf.astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAgiz9nEGTlk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](http://s5047.pcdn.co/wp-content/uploads/2015/10/confusion_matrix.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "h3MqiQOMr8IB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "metadata": {
        "id": "FcyqAPOE6Igm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 4.2 Traçando a curva de aprendizagem do método tendo em conta uma modificação sucessiva do tamanho dos dados de treino\n",
        "score_dt = []\n",
        "z = 0\n",
        "k = 0\n",
        "previous = 0\n",
        "\n",
        "for i in range(1, 10):\n",
        "  z = i/10\n",
        "  X_train, X_test, y_train, y_test = train_test_split(go_set, y, test_size = z, random_state = 5)\n",
        "  #Decision Tree Classifier\n",
        "  dt = DecisionTreeClassifier(random_state = 5)\n",
        "  dt.fit(X_train, y_train)\n",
        "  res_score = dt.score(X_test, y_test)\n",
        "  score_dt.append(res_score)\n",
        "  if(res_score > previous):\n",
        "    previous = res_score\n",
        "    k += 0.1\n",
        "\n",
        "display(go_set.info())\n",
        "    \n",
        "score_dt_local = score_dt\n",
        "print('\\n')\n",
        "plt.plot(score_dt)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Tamanho Dados Treino')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ntPMASfE4cYP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Decision Tree Classifier\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state = 5)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "dt_score = dt.score(X_test, y_test)\n",
        "print('Score de treino = ', dt.score(X_train, y_train))\n",
        "print('Score de teste = ', dt_score)\n",
        "\n",
        "# Contribuição das features\n",
        "rel_feature=pd.Series(dt.feature_importances_,X_test.columns).sort_values(ascending=False)\n",
        "rel_feature.head(10).plot(kind='bar')\n",
        "plt.show()\n",
        "\n",
        "display('A feature que mais contribuí para o resultado do algoritmo é:',rel_feature.head(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmOSftgTbf9l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 7. e 8. Matrix Confusão do Decision Tree\n",
        "y_preds = dt.predict(X_test)\n",
        "\n",
        "matrix_dt = confusion_matrix(y_test, y_preds)\n",
        "print(matrix_dt)\n",
        "\n",
        "import itertools \n",
        "classes = ['No', 'Yes']\n",
        "# plot confusion matrix\n",
        "plt.imshow(matrix_dt, interpolation='nearest', cmap=plt.cm.Greens)\n",
        "plt.title(\"Confusion Matrix do Decision Tree\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "fmt = 'd'\n",
        "thresh = matrix_dt.max() / 2.\n",
        "for i, j in itertools.product(range(matrix_dt.shape[0]), range(matrix_dt.shape[1])):\n",
        "    plt.text(j, i, format(matrix_dt[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if matrix_dt[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "\n",
        "print('\\n', classification_report(y_test, y_preds))\n",
        "\n",
        "print('\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pjd-Yx2inrKF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #@title Looping pelos subsets\n",
        "\n",
        "table_recall = []\n",
        "table_name = []\n",
        "table_precision = []\n",
        "go_set_loop = go_set.copy()\n",
        "\n",
        "for k in subsets:\n",
        "  go_set_loop = subsets.get(k)\n",
        "  features = list(go_set_loop.columns[0:49])\n",
        "  X = go_set_loop[features]\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(go_set, y, test_size = 0.2, random_state = 5)\n",
        "  print('\\n----Vamos ver os dados do subset', k,'----')\n",
        "  print('\\nFeatures de treino(instâncias, features) ', X_train.shape)\n",
        "  print('\\nFeatures de teste(instâncias, features) ', X_test.shape)\n",
        "  \n",
        "  loop = DecisionTreeClassifier(random_state = 5)\n",
        "  loop.fit(X_train, y_train)\n",
        "\n",
        "  loop_score = loop.score(X_test, y_test)\n",
        "  print('\\nScore de treino = ', loop.score(X_train, y_train))\n",
        "  print('Score de teste = ', loop_score, '\\n')\n",
        "  \n",
        "  y_preds = loop.predict(X_test)\n",
        "\n",
        "  matrix_loop = confusion_matrix(y_test, y_preds)  \n",
        "\n",
        "  classes = ['No', 'Yes']\n",
        "  # plot confusion matrix\n",
        "  plt.imshow(matrix_loop, interpolation='nearest', cmap=plt.cm.Greens)\n",
        "  plt.title(\"Confusion Matrix do Decision Tree\")\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  fmt = 'd'\n",
        "  thresh = matrix_loop.max() / 2.\n",
        "  for i, j in itertools.product(range(matrix_loop.shape[0]), range(matrix_loop.shape[1])):\n",
        "    plt.text(j, i, format(matrix_loop[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"blue\" if matrix_dt[i, j] > thresh else \"red\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "  print('\\n Matriz Confusão\\n', matrix_loop)\n",
        "  print('\\n', classification_report(y_test, y_preds))\n",
        "  table_name.append(k)\n",
        "  table_precision.append(precision_score(y_test, y_preds))\n",
        "  table_recall.append(recall_score(y_test, y_preds))\n",
        "  \n",
        "\n",
        "df=pd.DataFrame({'Subset': table_name, 'Recall': table_recall, 'Precision': table_precision})\n",
        "\n",
        "display(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdlMX6KhsD7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Random Forest Classifier"
      ]
    },
    {
      "metadata": {
        "id": "xnjb1yoW5zlA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 4.2 Traçando a curva de aprendizagem do método tendo em conta uma modificação sucessiva do tamanho dos dados de treino\n",
        "\n",
        "score_rf = []\n",
        "z = 0\n",
        "k = 0\n",
        "previous = 0\n",
        "\n",
        "for i in range(1, 10):\n",
        "  z = i/10\n",
        "  X_train, X_test, y_train, y_test = train_test_split(go_set, y, test_size = z, random_state = 5)\n",
        "  #Decision Tree Classifier\n",
        "  rf=RandomForestClassifier(n_estimators=20, max_depth=100, random_state = 5)\n",
        "  rf.fit(X_train, y_train)\n",
        "  res_score = rf.score(X_test, y_test)\n",
        "  score_rf.append(res_score)\n",
        "  if(res_score > previous):\n",
        "    previous = res_score\n",
        "    k += 0.1\n",
        "\n",
        "\n",
        "score_rf_local = score_rf  \n",
        "print('\\n')\n",
        "plt.plot(score_rf_local)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Tamanho Dados Treino')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "31nQNiLJ-0vj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Random Forest Classifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=20, max_depth=100, random_state = 5)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "rf_score = rf.score(X_test, y_test)\n",
        "print('Score de treino = ', rf.score(X_train, y_train))\n",
        "print('Score de teste = ', rf_score)\n",
        "\n",
        "# Contribuição das features\n",
        "rel_feature=pd.Series(rf.feature_importances_,X_test.columns).sort_values(ascending=False)\n",
        "rel_feature.head(10).plot(kind='bar')\n",
        "plt.show()\n",
        "\n",
        "display('A feature que mais contribuí para o resultado do algoritmo é:',rel_feature.head(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrV7RsIUgDt5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 7. e 8.  Matrix Confusão do Random Forest\n",
        "matrix_dt = 0\n",
        "y_preds = rf.predict(X_test)\n",
        "\n",
        "matrix_rf = get_confusion_matrix(y_test, y_preds)\n",
        "print(matrix_rf)\n",
        "\n",
        "import itertools \n",
        "classes = ['No', 'Yes']\n",
        "# plot confusion matrix\n",
        "plt.imshow(matrix_rf, interpolation='nearest', cmap=plt.cm.Greens)\n",
        "plt.title(\"Confusion Matrix do Decision Tree\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "fmt = 'd'\n",
        "thresh = matrix_rf.max() / 2.\n",
        "for i, j in itertools.product(range(matrix_rf.shape[0]), range(matrix_rf.shape[1])):\n",
        "    plt.text(j, i, format(matrix_rf[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if matrix_rf[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "\n",
        "print('\\n', classification_report(y_test, y_preds))\n",
        "\n",
        "print('\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "iIpfaEv9M3B2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Looping pelos subsets\n",
        "\n",
        "table_recall = []\n",
        "table_name = []\n",
        "table_precision = []\n",
        "go_set_loop = go_set.copy()\n",
        "\n",
        "for k in subsets:\n",
        "  go_set_loop = subsets.get(k)\n",
        "  features = list(go_set_loop.columns[0:49])\n",
        "  X = go_set_loop[features]\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(go_set, y, test_size = 0.2, random_state = 5)\n",
        "  print('\\n----Vamos ver os dados do subset', k,'----')\n",
        "  print('\\nFeatures de treino(instâncias, features) ', X_train.shape)\n",
        "  print('\\nFeatures de teste(instâncias, features) ', X_test.shape)\n",
        "  \n",
        "  loop = RandomForestClassifier(n_estimators=20, max_depth=20, random_state = 5)\n",
        "  loop.fit(X_train, y_train)\n",
        "\n",
        "  loop_score = loop.score(X_test, y_test)\n",
        "  print('\\nScore de treino = ', loop.score(X_train, y_train))\n",
        "  print('Score de teste = ', loop_score, '\\n')\n",
        "  \n",
        "  y_preds = loop.predict(X_test)\n",
        "\n",
        "  matrix_loop = get_confusion_matrix(y_test, y_preds)  \n",
        "\n",
        "  classes = ['No', 'Yes']\n",
        "  # plot confusion matrix\n",
        "  plt.imshow(matrix_loop, interpolation='nearest', cmap=plt.cm.Greens)\n",
        "  plt.title(\"Confusion Matrix do Decision Tree\")\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  fmt = 'd'\n",
        "  thresh = matrix_loop.max() / 2.\n",
        "  for i, j in itertools.product(range(matrix_loop.shape[0]), range(matrix_loop.shape[1])):\n",
        "    plt.text(j, i, format(matrix_loop[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"blue\" if matrix_loop[i, j] > thresh else \"red\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "  print('\\n Matriz Confusão\\n', matrix_loop)\n",
        "  print('\\n', classification_report(y_test, y_preds))\n",
        "  table_name.append(k)\n",
        "  table_precision.append(precision_score(y_test, y_preds))\n",
        "  table_recall.append(recall_score(y_test, y_preds))\n",
        "  \n",
        "\n",
        "df=pd.DataFrame({'Subset': table_name, 'Recall': table_recall, 'Precision': table_precision})\n",
        "\n",
        "display(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo5psFT2BuEx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 4.3 Comparação da curva de aprendizagem dos métodos\n",
        "\n",
        "plt.plot(score_dt, label='Decision Tree')\n",
        "plt.plot(score_rf, label='Random Forest')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Tamanho Dados Treino')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YekbtQmQvqQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##K-Nearest-Neighbors"
      ]
    },
    {
      "metadata": {
        "id": "7L_Hb41YqKG1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title KNN Classifier\n",
        "\n",
        "\n",
        "kn = KNeighborsClassifier(n_neighbors=5)\n",
        "kn = kn.fit(X_train, y_train.values)\n",
        "\n",
        "score_train = kn.score(X_train, y_train)\n",
        "score_test = kn.score(X_test, y_test)                              \n",
        "\n",
        "print('Score de treino = ', score_train)\n",
        "print('Score de teste = ', score_test, '\\n')\n",
        "expected = y_test\n",
        "predicted = kn.predict(X_test)\n",
        "\n",
        "\n",
        "# Preparar plot\n",
        "k_range = range(1, 26)\n",
        "\n",
        "scores = []\n",
        "\n",
        "# Loop\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    scores.append(accuracy_score(y_test, predicted))\n",
        "\n",
        "display(scores)\n",
        "print('\\n')\n",
        "\n",
        "plt.plot(k_range, scores)\n",
        "plt.xlabel('Número de Neighbors')\n",
        "plt.ylabel('Assertividade to Teste')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CTSwVQ2HwmaI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Perceptron\n",
        "\n",
        "#Importar o módulo\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "#Fazer o split do treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 5)\n",
        "\n",
        "\n",
        "#FOR que verifica quais os settings ideais de treino para o perceptron\n",
        "melhor_n_iter = 0\n",
        "melhor_eta = 0\n",
        "melhor_accuracy = 0\n",
        "melhor_tolerancia = 0\n",
        "accuracyAtual = 0\n",
        "z = 0\n",
        "w = 1\n",
        "\n",
        "for i in range(20, 25):\n",
        "  for j in range(1, 10):\n",
        "    w = 1\n",
        "    z = j/100\n",
        "    for k in range(1,10):\n",
        "      ppnTemp = Perceptron(max_iter=i, eta0=z, random_state=1, tol=0.1/w)\n",
        "      ppnTemp.fit(X_train, y_train)\n",
        "      y_pred = ppnTemp.predict(X_test)\n",
        "      accuracyAtual = accuracy_score(y_test, y_pred)\n",
        "      if(accuracyAtual > melhor_accuracy):\n",
        "        melhor_accuracy = accuracyAtual\n",
        "        melhor_n_iter = i\n",
        "        melhor_eta = z\n",
        "        melhor_tolerancia = 0.1/w\n",
        "      w = w*10\n",
        "    j += 1\n",
        "  i += 1\n",
        "\n",
        "\n",
        "print('melhor accuracy: ')\n",
        "print(melhor_accuracy)\n",
        "print('com o numero iteracoes:')\n",
        "print(melhor_n_iter)\n",
        "print('melhor multiplicador:')\n",
        "print(melhor_eta)\n",
        "print('e tolerancia:')\n",
        "print(melhor_tolerancia)\n",
        "\n",
        "\n",
        "#Criar um perceptron com as definições ideais\n",
        "ppn = Perceptron(max_iter=melhor_n_iter, eta0=melhor_eta, random_state=5, tol=melhor_tolerancia)\n",
        "\n",
        "# Treinar o perceptron\n",
        "ppn.fit(X_train, y_train)\n",
        "\n",
        "#Predizer resultados com dados de teste\n",
        "y_pred = ppn.predict(X_test)\n",
        "\n",
        "#Comparação de dados\n",
        "#Dados inferidos pelo Perceptron em testes\n",
        "print('Dados inferidos perceptron:')\n",
        "print(y_pred)\n",
        "\n",
        "#Dados reais\n",
        "print('Dados reais:')\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTOY4ysV7BA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9.Definir um contexto de uso razoável do algoritmo e perceber se nessa situação é mais relevante a precisão ou o recall\n",
        "\n",
        "Neste caso não é relevante obter resultados muito precisos, o ideal é ter um bom balanço entre a precision e o recall.\n",
        "\n",
        "Apenas os dataset data_work e data_social obtêm este balanço tal como é visível no loop da confusion matrix dos dois algoritmos."
      ]
    },
    {
      "metadata": {
        "id": "x4O3c7ND7Pc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 10. De acordo com a métrica encontrada, rever feautures usadas e procurar melhorar o resultado obtido, testando mais/outras features;\n",
        "\n",
        "O dataset com 59 features foi subdividido por áreas e o melhor resultado obtido é somente a conjugação entre os dois datasets que melhor balançam a precision e o recall."
      ]
    }
  ]
}